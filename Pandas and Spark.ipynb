{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb5d65ee-469c-4478-a85d-3d198e195093",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#  Pandas vs. PySpark: Basic Function Comparison\n",
    "\n",
    "\n",
    "## ðŸ“ Sample Dataset\n",
    "\n",
    "**sample.csv**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae56fad-2b2b-42b0-9a93-8a822f0a8631",
   "metadata": {},
   "source": [
    "# ðŸ“Š Pandas vs. PySpark: Basic Function Comparison\n",
    "\n",
    "This notebook shows a side-by-side comparison of **Pandas** and **PySpark** for common data manipulation tasks using the same dataset. Useful for understanding how operations translate between small-scale (Pandas) and big data (PySpark) processing.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Sample Dataset\n",
    "\n",
    "We'll use the following dataset:\n",
    "\n",
    "```\n",
    "name,age,salary\n",
    "Alice,25,50000\n",
    "Bob,30,60000\n",
    "Charlie,35,70000\n",
    "David,40,80000\n",
    "Eva,45,90000\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 1. Load CSV File\n",
    "\n",
    "###  Pandas\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"sample.csv\")\n",
    "print(df)\n",
    "```\n",
    "\n",
    "###  PySpark\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"BasicComparison\").getOrCreate()\n",
    "df = spark.read.csv(\"sample.csv\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 2. Show Schema / Info\n",
    "\n",
    "###  Pandas\n",
    "\n",
    "```python\n",
    "df.info()\n",
    "```\n",
    "\n",
    "###  PySpark\n",
    "\n",
    "```python\n",
    "df.printSchema()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 3. Basic Statistics\n",
    "\n",
    "###  Pandas\n",
    "\n",
    "```python\n",
    "df.describe()\n",
    "```\n",
    "\n",
    "###  PySpark\n",
    "\n",
    "```python\n",
    "df.describe().show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 4. Filter Rows (age > 30)\n",
    "\n",
    "###  Pandas\n",
    "\n",
    "```python\n",
    "df[df['age'] > 30]\n",
    "```\n",
    "\n",
    "###  PySpark\n",
    "\n",
    "```python\n",
    "df.filter(df.age > 30).show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 5. Select Specific Columns\n",
    "\n",
    "###  Pandas\n",
    "\n",
    "```python\n",
    "df[['name', 'salary']]\n",
    "```\n",
    "\n",
    "###  PySpark\n",
    "\n",
    "```python\n",
    "df.select(\"name\", \"salary\").show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 6. GroupBy and Aggregate (Average Salary by Age)\n",
    "\n",
    "###  Pandas\n",
    "\n",
    "```python\n",
    "df.groupby(\"age\").mean()\n",
    "```\n",
    "\n",
    "###  PySpark\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df.groupBy(\"age\").agg(avg(\"salary\")).show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 7. Add a New Column (bonus = salary * 0.1)\n",
    "\n",
    "###  Pandas\n",
    "\n",
    "```python\n",
    "df[\"bonus\"] = df[\"salary\"] * 0.1\n",
    "df.head()\n",
    "```\n",
    "\n",
    "###  PySpark\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"bonus\", col(\"salary\") * 0.1)\n",
    "df.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 8. Sorting by Salary (Descending)\n",
    "\n",
    "###  Pandas\n",
    "\n",
    "```python\n",
    "df.sort_values(\"salary\", ascending=False)\n",
    "```\n",
    "\n",
    "###  PySpark\n",
    "\n",
    "```python\n",
    "df.orderBy(\"salary\", ascending=False).show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##  Summary Table\n",
    "\n",
    "| Operation | Pandas | PySpark |\n",
    "| --- | --- | --- |\n",
    "| Read CSV | `pd.read_csv()` | `spark.read.csv()` |\n",
    "| Show Schema | `df.info()` | `df.printSchema()` |\n",
    "| Filter | `df[df['age'] > 30]` | `df.filter(df.age > 30)` |\n",
    "| Select Columns | `df[['name', 'salary']]` | `df.select(\"name\", \"salary\")` |\n",
    "| GroupBy & Aggregate | `df.groupby().mean()` | `df.groupBy().agg()` |\n",
    "| Add Column | `df[\"new\"] = ...` | `df.withColumn()` |\n",
    "| Sort | `df.sort_values()` | `df.orderBy()` |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae735404-bbaa-433f-94fc-8d24cb50a4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"BasicComparison\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92e8cc10-06a7-4b5b-a569-4092ac9dc420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicComparison\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.appName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74342aa8-3a3d-4511-a277-ebd7630c1d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "712e76c8-f443-44dd-a7a4-2c66e8fb5b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicComparison\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.appName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9240d5e6-9311-4987-8527-7017884d9b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "App Name: BasicComparison\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApp Name:\u001b[39m\u001b[38;5;124m\"\u001b[39m, spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39mappName)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApp ID:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapplicationId\u001b[49m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaster:\u001b[39m\u001b[38;5;124m\"\u001b[39m, spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39mmaster)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpark UI:\u001b[39m\u001b[38;5;124m\"\u001b[39m, spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39muiWebUrl)\n",
      "File \u001b[1;32mC:\\BELAJAR_HADOOP\\spark-3.5.6-bin-hadoop3\\python\\pyspark\\context.py:585\u001b[0m, in \u001b[0;36mSparkContext.applicationId\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapplicationId\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;124;03m    A unique identifier for the Spark application.\u001b[39;00m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;124;03m    Its format depends on the scheduler implementation.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;124;03m    'local-...'\u001b[39;00m\n\u001b[0;32m    584\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m()\u001b[38;5;241m.\u001b[39mapplicationId()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
     ]
    }
   ],
   "source": [
    "print(\"App Name:\", spark.sparkContext.appName)\n",
    "print(\"App ID:\", spark.sparkContext.applicationId)\n",
    "print(\"Master:\", spark.sparkContext.master)\n",
    "print(\"Spark UI:\", spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb68c25-900b-4b36-8def-34996992e6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
